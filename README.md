# AI-Interview-Code
AI 大模型相关算法中手写的面试题，（非 LeetCode），一般比 LeetCode 更考察一个人的综合能力，又更贴近业务和基础知识一点

欢迎关注我的博客：[用代码打点酱油](https://bruceyuan.com/) ，内容一般会首先更新到博客上面。

## 目录

| 题目 | 难度 | 知识点 | 文字解读 | 视频解读 |
| ---- | ---- | ---- | ---- | ---- |
| 手写 Self-Attention | ⭐⭐⭐ | 注意力机制 | [手写 Self-Attention 的四重境界](https://bruceyuan.com/hands-on-code/from-self-attention-to-multi-head-self-attention.html#%E9%9D%A2%E8%AF%95%E5%86%99%E6%B3%95-%E5%AE%8C%E6%95%B4%E7%89%88-%E6%B3%A8%E6%84%8F%E6%B3%A8%E9%87%8A) |  [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV19YbFeHETz)](https://www.bilibili.com/video/BV19YbFeHETz/)<br />[![Youtube](https://img.shields.io/youtube/views/d_jwwnYCzIg?style=social)](https://www.youtube.com/watch?v=d_jwwnYCzIg)  |
| 手写 Multi-Head Self-Attention | ⭐⭐⭐ | 注意力机制 | [手写 Multi-Head Self-Attention](https://bruceyuan.com/hands-on-code/from-self-attention-to-multi-head-self-attention.html#%E9%9D%A2%E8%AF%95%E5%86%99%E6%B3%95-%E5%AE%8C%E6%95%B4%E7%89%88-%E6%B3%A8%E6%84%8F%E6%B3%A8%E9%87%8A) | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV19mxdeBEbu)](https://www.bilibili.com/video/BV19mxdeBEbu/)<br />[![Youtube](https://img.shields.io/youtube/views/SsWxatYLB-s?style=social)](https://www.youtube.com/watch?v=SsWxatYLB-s) |
| 手写 Transformer Decoder（Causal Language Model, LM）| ⭐⭐⭐⭐ | Transformer 架构 | [手写 Transformer Decoder](https://bruceyuan.com/hands-on-code/hands-on-causallm-decoder.html) | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Nh1QYCEsS)](https://www.bilibili.com/video/BV1Nh1QYCEsS/)<br />[![Youtube](https://img.shields.io/youtube/views/yzEotGJaQ74?style=social)](https://www.youtube.com/watch?v=yzEotGJaQ74)  |
| 计算 LLM (Decoder) 模型的参数量 | ⭐⭐⭐ | 模型参数量 | TODO | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Zw4ue2ELg)](https://www.bilibili.com/video/BV1Zw4ue2ELg/)<br />[![Youtube](https://img.shields.io/youtube/views/q5quYPt2z5s?style=social)](https://www.youtube.com/watch?v=q5quYPt2z5s)  |
| LLM 模型训练推理显存占用预估 | ⭐⭐⭐⭐ | 显存占用 | [LLM 大模型训练-推理显存占用分析](https://bruceyuan.com/post/llm-train-infer-memoery-usage-calculation.html) |  |
| 手写 AUC | ⭐⭐ | 模型评估 | TODO |  |
| 手写 KMeans | ⭐⭐⭐⭐⭐ | 聚类 | TODO |  |
| 手写 线性回归 | ⭐⭐⭐⭐⭐ | 线性回归 | TODO |  |
| 手写 BPE 分词器 | ⭐⭐⭐⭐⭐ | 分词 | TODO |  |
| 手写 LayerNorm | ⭐⭐ | 归一化 | TODO |  |
| 手写 BatchNorm | ⭐⭐ | 归一化 | TODO |  |
| 手写 Softmax | ⭐⭐ | 激活函数 | TODO |  |
